{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Airlines',\n",
       " 'Total_Stops',\n",
       " 'Price',\n",
       " 'Duration',\n",
       " 'Destination',\n",
       " 'Date',\n",
       " 'Airlines_list',\n",
       " 'Pays_Destination',\n",
       " 'Depart']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertir les variables catégorielles en indices numériques\n",
    "indexers = [StringIndexer(inputCol=column,   outputCol=column + \"_index\").fit(df) \n",
    "            for column in [\"Airlines\",\"Date\", \"Destination\"]]\n",
    "\n",
    "\n",
    "# Appliquer chaque StringIndexer au DataFrame\n",
    "for indexer in indexers:\n",
    "    df = indexer.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Airlines',\n",
       " 'Total_Stops',\n",
       " 'Price',\n",
       " 'Duration',\n",
       " 'Destination',\n",
       " 'Date',\n",
       " 'Airlines_list',\n",
       " 'Pays_Destination',\n",
       " 'Depart',\n",
       " 'Airlines_index',\n",
       " 'Date_index',\n",
       " 'Destination_index']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[ \"Airlines_index\",\"Date_index\", \"Destination_index\", \"Total_Stops\", \"Duration\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Airlines',\n",
       " 'Total_Stops',\n",
       " 'Price',\n",
       " 'Duration',\n",
       " 'Destination',\n",
       " 'Date',\n",
       " 'Airlines_list',\n",
       " 'Pays_Destination',\n",
       " 'Depart',\n",
       " 'Airlines_index',\n",
       " 'Date_index',\n",
       " 'Destination_index',\n",
       " 'features']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop()\n",
    "df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colonnes nécessaires pour l'entraînement\n",
    "df = df.select(col(\"features\"), col(\"Price\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/01 15:00:50 WARN DAGScheduler: Broadcasting large task binary with size 1052.6 KiB\n",
      "24/12/01 15:00:50 WARN DAGScheduler: Broadcasting large task binary with size 1744.1 KiB\n",
      "24/12/01 15:00:51 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/12/01 15:00:54 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "24/12/01 15:00:56 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "24/12/01 15:00:57 WARN DAGScheduler: Broadcasting large task binary with size 1293.9 KiB\n",
      "24/12/01 15:01:00 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB\n",
      "24/12/01 15:01:00 WARN DAGScheduler: Broadcasting large task binary with size 1718.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 81.8292703240851\n",
      "R2: 0.8481208313507199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 1. Création du Random Forest\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"label\",  \n",
    "    numTrees=100,      # nombre d'arbres\n",
    "    maxDepth=10,       # profondeur maximale des arbres\n",
    "    seed=42 ,          # reproductibilité\n",
    "    maxBins=210 #nombre de valeurs unique\n",
    ")\n",
    "\n",
    "# 2. Séparation train/test\n",
    "(train_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 3. Entraînement du modèle\n",
    "model = rf.fit(train_data)\n",
    "\n",
    "# 4. Prédictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# 5. Évaluation\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2: {r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prédire les prix pour de nouvelles données\n",
    "new_data = spark.createDataFrame(\n",
    "    [(\"Vueling, Iberia\",1, \"Madrid\", 5.3333335,\"2025-06-29\")],\n",
    "    [\"Airlines\",\"Total_Stops\", \"Destination\", \"Duration\", \"Date\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        prediction|\n",
      "+------------------+\n",
      "|145.12609545696793|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformer les nouvelles données en utilisant les mêmes StringIndexers et assembler\n",
    "for indexer in indexers:\n",
    "    new_data = indexer.transform(new_data)\n",
    "\n",
    "new_data = assembler.transform(new_data)\n",
    "new_data = new_data.select(\"features\")\n",
    "\n",
    "# Prédire le prix pour les nouvelles données\n",
    "predicted_price = model.transform(new_data)\n",
    "predicted_price.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/01 15:04:18 WARN TaskSetManager: Stage 86 contains a task of very large size (1011 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Model Save\n",
    "\n",
    "model.save(\"./model/rf_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
